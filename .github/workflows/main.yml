name: MLOps CI

on:
  push:
    branches: ["main"]
  pull_request:

jobs:
  build-train-test:
    runs-on: ubuntu-latest

    env:
      PYTHON_VERSION: "3.11"
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      MLFLOW_S3_ENDPOINT_URL: "http://localhost:9000"
      MLFLOW_TRACKING_URI: "http://localhost:5000"

    services:
      minio:
        image: minio/minio:latest
        ports:
          - 9000:9000
          - 9001:9001
        env:
          MINIO_ROOT_USER: ${{ secrets.AWS_ACCESS_KEY_ID }}
          MINIO_ROOT_PASSWORD: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        options: >-
          --health-cmd "curl -f http://localhost:9000/minio/health/live || exit 1"
          --health-interval 5s
          --health-timeout 3s
          --health-retries 20

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install dvc[s3] optuna mlflow scikit-learn pandas joblib requests fastapi "uvicorn[standard]"

      - name: Create MinIO bucket (mlflow)
        run: |
          docker run --rm --network host minio/mc:latest sh -c '
            mc alias set local http://localhost:9000 $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY &&
            mc mb --ignore-existing local/mlflow &&
            echo "Bucket mlflow OK"
          '

      - name: Start MLflow server (Docker)
        run: |
          docker run -d --name mlflow --network host \
            -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
            -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
            -e MLFLOW_S3_ENDPOINT_URL=$MLFLOW_S3_ENDPOINT_URL \
            ghcr.io/mlflow/mlflow:latest \
            mlflow server \
              --host 0.0.0.0 --port 5000 \
              --backend-store-uri sqlite:////tmp/mlflow.db \
              --default-artifact-root s3://mlflow
          sleep 3
          curl -f http://localhost:5000

      - name: DVC pull data (if remote configured)
        run: |
          dvc --version
          dvc pull || echo "No DVC remote / nothing to pull"

      - name: Train baseline + export models
        run: |
          python src/serving/export_models.py

      - name: Start API (uvicorn) & test /health
        run: |
          export MODEL_PATH=models/production/model.joblib
          nohup uvicorn src.serving.app:app --host 0.0.0.0 --port 8000 >/tmp/api.log 2>&1 &
          sleep 3
          curl -f http://localhost:8000/health
          tail -n 60 /tmp/api.log
          pkill -f "uvicorn src.serving.app:app" || true

      - name: Build Docker image (serving)
        run: |
          docker build -f docker/serving.Dockerfile -t breastcancer-api:ci .

      - name: Run container & test /health
        run: |
          docker run -d --name api -p 8000:8000 breastcancer-api:ci
          sleep 3
          curl -f http://localhost:8000/health
          docker logs api --tail=80

      - name: Cleanup
        if: always()
        run: |
          docker rm -f mlflow api || true
